---
title: "Aula7_NLP"
author: "Ricardo Primi"
date: "11/8/2020"
output: html_document
---
## Problema

## Trabalhando com textos usando o `tidytext`

* https://www.tidytextmining.com/index.html


### Bibliotecas
```{r}

 library(readxl)
 library(tidyverse)


```

### Dados
```{r}
 
 frameworks <- read_excel("../dados/sems_definitions.xlsx") 

```

### Explorando dados
```{r}

library(sjmisc)

frq(frameworks$system)
frq(frameworks$domain)
frq(frameworks$facet)


```

### Preparação dos dados para tokenização

```{r}

library(stringr)

  frameworks <- frameworks %>%
    mutate(
      domain = str_to_lower(domain),
      facet  = str_to_lower(facet),
      definition = str_to_lower(definition)
  )

  frameworks$definition[1:3] %>% paste(collapse =  " ")
  
  frameworks2 <- frameworks %>% 
    group_by(system, facet) %>%
    summarise(
      domain = first(domain),
      definition = paste(definition, collapse = " ")
    ) 
 
```

  
### Tokenize
```{r}

 library(tidytext)

# Tokenize palvras 
  frameworks3 <- frameworks2 %>% 
    ungroup  %>%
    unnest_tokens(words, definition)

  
 length(unique(frameworks3$words))


```
### Explorando as palavras
```{r}

frameworks3  %>% count(words, sort = TRUE)

```

### Stop words and frequência de palavras

Snowball: http://snowball.tartarus.org/algorithms/english/stop.txt
NLTK: http://www.nltk.org/
Stopwords ISO: https://github.com/stopwords-iso


```{r}

  words <-  unique(
    c(stopwords::data_stopwords_snowball$en,
      stopwords::data_stopwords_nltk$en,
      stopwords::data_stopwords_stopwordsiso$en
      )
  )

  stopwords  <-tibble(
    words = words
  )

  
# Palavras gerais   
    frameworks3 %>%
        count(words, sort = TRUE) %>%
        filter(n > 50 ) %>%
        mutate(words = reorder(words, n)) %>%
       ggplot(aes(words, n)) +
        geom_col() +
        xlab(NULL) +
        coord_flip()
 
       
# Remove stopwords
     frameworks3 %>%
        count(words, sort = TRUE) %>%
        anti_join(stopwords) %>%
        filter(n > 15)  %>%
        mutate(word = reorder(words, n)) %>%
        ggplot(aes(word, n)) +
        geom_col() +
        xlab(NULL) +
        coord_flip()

# Word cloud   
   library(wordcloud)

   frameworks3 %>%
        anti_join(stopwords) %>%
        count(words) %>%
        with(wordcloud(words, n, 
          colors = brewer.pal(12, "Set1"),
            max.words = 100)) 

    stopwords[221, "words"] <- "pra"
    stopwords[222, "words"] <- "dá"


```

### Análise de similaridade usando o word embeddings 

* Faca o download do arquivo: 
http://nlp.stanford.edu/data/glove.6B.zip

De Stanford https://nlp.stanford.edu/projects/glove/


```{r}

  library(readr)

 # Lê word embeddings
  glove_wv_300 <- read_delim(
    file = "../../glove.6B.300d.txt", 
    delim = " ",quote="", 
    skip = 1,  
    col_names = FALSE, 
    progress = TRUE)
  
  names(glove_wv_300)[1]<-"words"
  names(glove_wv_300)[2:301]<-paste("d", 1:300, sep= "")
  
  glove_wv_300[2000:4000, ] %>% view

```

```{r}

 frameworks4 <- frameworks3 %>% 
  left_join( glove_wv_300, by = "words") %>%
  filter(!is.na(d1))

 save(frameworks4, file = "frameworks4.RData")

```

### Visualizando clusters usando tsn-e 
* https://lvdmaaten.github.io/tsne/
* https://distill.pub/2016/misread-tsne/

```{r}

  library(Rtsne)

  frameworks5 <- frameworks4 %>% 
    group_by(system, facet) %>%
    summarise(
      across(d1:d300, mean, na.rm=TRUE)
     )
  
  tsne <- frameworks5 %>% 
  select(3:302) %>%
  Rtsne( perplexity = 13) 
  
  frameworks5 <- frameworks5 %>% bind_cols(as.data.frame(tsne$Y))
  
  
  names(frameworks5)
  
   library(ggrepel)
  library(ggthemes)
  library(RColorBrewer)
  
  ggplot(data = frameworks5,  
            mapping = aes(
             y = V1,
             x = V2,
            color = system) 
           ) +
          
         geom_point()  +
                  
         geom_text_repel(
            aes(label=facet), 
            size=2, vjust=-1.2
            ) +
       theme_minimal() +
      scale_color_brewer() +
       theme(legend.position = "none") 
  
   frameworks5 %>%
    filter( system == "MACM") %>%
    ggplot( mapping = aes(
             y = V1,
             x = V2) 
           ) +
          
         geom_point()  +
                  
         geom_text_repel(
            aes(label=facet), 
            size=2, vjust=-1.2
            ) +
       theme_minimal() +
      scale_color_brewer() +
       theme(legend.position = "none") 
  
  
   frameworks5 %>%
    filter( system == "OECD") %>%
    ggplot( mapping = aes(
             y = V1,
             x = V2) 
           ) +
          
         geom_point()  +
                  
         geom_text_repel(
            aes(label=facet), 
            size=2, vjust=-1.2
            ) +
       theme_minimal() +
      scale_color_brewer() +
       theme(legend.position = "none") 


```

### Latent Semantic Analyais via Topic modeling (Latent Dirichlet Allocation - LDA )
* Usaremos o pacote text2vec
* Site: http://text2vec.org/topic_modeling.html
* Usando palavras


* Criando a matriz termo-documento 

```{r}
  # install.packages('servr') 
  library(stringr)
  library(text2vec)
 
  frameworks2$rwn_id <- as.integer(rownames(frameworks2))

  prep_fun <- tolower
  tok_fun <- word_tokenizer

  it = itoken(
      frameworks2$definition, 
      ids = frameworks2$rwn_id, 
      preprocessor = prep_fun, 
      tokenizer = tok_fun, 
      progressbar = FALSE
      )
  
  vocab <- create_vocabulary(it,
      stopwords = stopwords$words) %>%
     prune_vocabulary(
      term_count_min = 1, 
      doc_proportion_max = 0.8
      )
  
  glimpse(vocab)
  
  vocab$term
  vocab$term_count
  vocab$doc_count %>% frq
  
  
  vectorizer <- vocab_vectorizer(vocab)
  
  dtm = create_dtm(it, vectorizer)
  
  dim(dtm)

  # define tfidf model
  tfidf = TfIdf$new()
  
  # fit model to train data and transform train data with fitted model
  dtm_tfidf = fit_transform(dtm, tfidf)

 lda_model = LDA$new(
   n_topics = 12, 
   doc_topic_prior = 0.1,
   topic_word_prior = 0.01
   )

  doc_topic_distr = 
    lda_model$fit_transform(
    x = dtm_tfidf, 
    n_iter = 1000, 
    convergence_tol = 0.001, 
    n_check_convergence = 25, 
    progressbar = FALSE
    )
 
  lda_model$get_top_words(n = 12, 
    topic_number = c(1:12), lambda = 0.2)
  
    
  lda_model$plot()
    
  
```

###### LDA usando bigramas 2 a 3


```{r}

  it = itoken(
      frameworks2$definition, 
      ids = frameworks2$rwn_id, 
      preprocessor = prep_fun, 
      tokenizer = tok_fun, 
      progressbar = FALSE
      )
  
  vocab = create_vocabulary(
    it, 
    stopwords = stopwords$words,
    ngram = c(3, 3)
    ) %>%
    prune_vocabulary(
      term_count_min = 1, 
      doc_proportion_max = 0.8)
  
  dim(vocab)
  
  vectorizer = vocab_vectorizer(vocab)
  dtm = create_dtm(it, vectorizer)
  
  dim(dtm)

  # define tfidf model
  tfidf = TfIdf$new()
  # fit model to train data and transform train data with fitted model
  dtm_tfidf = fit_transform(dtm, tfidf)

  dim(dtm_tfidf )

 lda_model = LDA$new(
   n_topics =8, 
   doc_topic_prior = 0.1,
   topic_word_prior = 0.01
   )

  doc_topic_distr = 
    lda_model$fit_transform(
    x = dtm_tfidf, n_iter = 1000, 
    convergence_tol = 0.001, 
    n_check_convergence = 25, 
    progressbar = FALSE
    )
   
  barplot(
    doc_topic_distr[2, ], xlab = "topic", 
    ylab = "proportion", ylim = c(0, 1), 
    names.arg = 1:ncol(doc_topic_distr)
    )
    
  lda_model$get_top_words(n = 12, 
    topic_number = c(1:4), lambda = 0.2)
  
    
  lda_model$plot()
 
```


#### Term’s inverse document frequency (idf)

_Term’s inverse document frequency (idf), which decreases the weight for commonly used words and increases the weight for words that are not used very much in a collection of documents. This can be combined with term frequency to calculate a term’s tf-idf (the two quantities multiplied together), the frequency of a term adjusted for how rarely it is used_ (p. 31)

_The idea of tf-idf is to find the important words for the content of each document by decreasing the weight for commonly used words and increasing the weight for words that are not used very much in a collection or corpus of documents_ (p. 37)

$$idf_{termo} = \mbox{ln} \frac{n_{documentos}}{n_{documentos.contendo.termo}}$$
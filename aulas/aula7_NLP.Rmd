---
title: "Aula7_NLP"
author: "Ricardo Primi"
date: "11/8/2020"
output: html_document
---
## Problema

## Trabalhando com textos usando o `tidytext`

* https://www.tidytextmining.com/index.html


### Bibliotecas
```{r}
 library(readxl)
 library(tidyverse)

```

### Dados
```{r}
 
 frameworks <- read_excel("../dados/sems_definitions.xlsx") 

```

### Explorando dados
```{r}

  library(sjmisc)
  
  frq(frameworks$system)
  frq(frameworks$domain)
  frq(frameworks$facet)
  
  length(unique(frameworks$facet))

```

### Preparação dos dados para tokenização

```{r}

 library(stringr)

  frameworks <- frameworks %>%
    mutate(
      domain = str_to_lower(domain),
      facet  = str_to_lower(facet),
      definition = str_to_lower(definition)
  )

  frameworks$definition[1:3] %>% paste( collapse = " ")
  
  frameworks2 <- frameworks %>% 
    group_by(system, facet) %>%
    summarise(
      domain = first(domain),
      definition = paste(definition, collapse = " ")
    ) 
 
```

  
### Tokenize
```{r}

 library(tidytext)

# Tokenize palvras 
  frameworks3 <- frameworks2 %>% 
    select(system, facet, definition) %>%
    ungroup  %>%
    unnest_tokens(words, definition)

  
  length(unique(frameworks3$words))


```
### Explorando as palavras
```{r}

 frameworks3  %>% count(words, sort = TRUE)

```

### Stop words and frequência de palavras

Snowball: http://snowball.tartarus.org/algorithms/english/stop.txt
NLTK: http://www.nltk.org/
Stopwords ISO: https://github.com/stopwords-iso


```{r}

  words <-  unique(
    c(stopwords::data_stopwords_snowball$en,
      stopwords::data_stopwords_nltk$en,
      stopwords::data_stopwords_stopwordsiso$en
      )
   )

  stopwords  <-tibble(
    words = words
  )

  
# Palavras gerais   
  frameworks3 %>%
        count(words, sort = TRUE) %>%
        filter(n > 50 ) %>%
       mutate(words = reorder(words, n)) %>%
       ggplot(aes(words, n)) +
        geom_col() +
        xlab(NULL) +
        coord_flip()
 
    
       
# Remove stopwords
     frameworks3 %>%
        count(words, sort = TRUE) %>%
        anti_join(stopwords) %>%
        filter(n > 15)  %>%
        mutate(word = reorder(words, n)) %>%
        ggplot(aes(word, n)) +
        geom_col() +
        xlab(NULL) +
        coord_flip()

# Word cloud   
   library(wordcloud)

   temp <-  frameworks3 %>%
        anti_join(stopwords) %>%
        count(words)  %>%
        with(
           wordcloud(words, n, 
          colors = brewer.pal(12, "Set1"),
          max.words = 100)
        )
 

```

### Análise de similaridade usando o word embeddings 

* Faca o download do arquivo: 
http://nlp.stanford.edu/data/glove.6B.zip

De Stanford https://nlp.stanford.edu/projects/glove/


```{r}

  library(readr)

 # Lê word embeddings
  glove_wv_300 <- read_delim(
    file = "../../glove.6B.300d.txt", 
    delim = " ",quote="", 
    skip = 1,  
    col_names = FALSE, 
    progress = TRUE)
  
  names(glove_wv_300)[1] <-"words"
  names(glove_wv_300)[2:301]<-paste("d", 1:300, sep= "")
  
  glove_wv_300[2000:4000, ] %>% view

```

```{r}

 load("~/Dropbox (Personal)/Ciencia de dados/ds_psicom/aulas/frameworks4.RData")

```



```{r}

 frameworks4 <- frameworks3 %>% 
  left_join( glove_wv_300, by = "words") %>%
  filter(!is.na(d1))

 save(frameworks4, file = "frameworks4.RData")
 
```

### Visualizando clusters usando tsn-e 


* https://lvdmaaten.github.io/tsne/
* https://distill.pub/2016/misread-tsne/

Efeito do hiperparâmetro "perplexity": 
* https://distill.pub/2016/misread-tsne/

* Retirando palavras duplicadas e stopwords e preparando os documentos

```{r}

 
  frameworks4[ , c(1, 2, 4)] %>% 
    arrange(system, facet, words) %>%
    view
 
  s <- duplicated(frameworks4[ , c(1, 2, 4)])
  s %>% table

  frameworks5 <- frameworks4 %>% 
    filter(! duplicated(frameworks4[ , c(1, 2, 4)]) ) %>%
    anti_join(stopwords) %>%
    group_by(system, facet) %>%
    summarise(
      across(d1:d300, mean, na.rm=TRUE)
     ) %>%
   mutate(
     documento = str_wrap(paste(system, facet, sep = "_"), width = 23 ),
     system = factor(system)
   )
  
 

```


* Redução de dimensionalidade via tsne

```{r}
 
  library(Rtsne)

  names(frameworks5)
  
  tsne <- frameworks5 %>% 
  ungroup() %>%
  select(3:302) %>%
  Rtsne( perplexity = 8) 
  
  tsne$Y
  
  frameworks5 <- frameworks5 %>% bind_cols(as.data.frame(tsne$Y))
  
  names(frameworks5)
  
  library(ggrepel)
  library(ggthemes)
  library(RColorBrewer)
  library(scales)
  
  unique(frameworks5$system)
  
 # install.packages('unikn')  
  library(unikn)        
  

  cores18 <-  c(
    pal_unikn_pair, pal_unikn_web[1:2]
    ) %>% 
    flatten_chr
  
  show_col(cores18)
  
  show_col(RColorBrewer::brewer.pal(18, "Set1"))
  show_col(RColorBrewer::brewer.pal(9, "Spectral"))
  show_col(RColorBrewer::brewer.pal(12, "Paired"))
  
  cores18 <-  c(
    RColorBrewer::brewer.pal(9, "Spectral"), 
    RColorBrewer::brewer.pal(9, "Paired")
    ) 
  
  getwd()
  
  f1 <- ggplot(data = frameworks5,  
    mapping = aes(
      y = V1,
      x = V2,
      color = system) 
      ) +
    geom_point()  +
    geom_text_repel(
      aes(label=documento), 
        size=2, vjust=-1.2
        ) +
    theme_minimal() +
    scale_color_manual(values = cores18) +
    theme(legend.position = "none") 
  
  ggsave(filename = "f1.jpeg", device = "jpeg",
         width = 29, height = 21, units = "cm" )
  
  


```

* Visualizando sistemas

```{r}

  
   f2 <- frameworks5 %>%
    filter( (system == "MACM" |  system == "OECD")) %>%
    ggplot( mapping = aes(
             y = V1,
             x = V2,
             color = system
             ) 
           ) +
          
         geom_point()  +
                  
         geom_text_repel(
            aes(label=facet), 
            size=2, vjust=-1.2
            ) +
       theme_minimal() +
    scale_color_manual(values = cores18[1:2]) +
    theme(legend.position = "none") +
  
  
  ggsave(filename = "f2.jpeg", device = "jpeg",
         width = 29, height = 21, units = "cm" )
  
  
```



### Latent Semantic Analyais via Topic modeling (Latent Dirichlet Allocation - LDA )

* Usaremos o pacote text2vec
* Site: http://text2vec.org/topic_modeling.html
* Usando palavras

* Criando a matriz termo-documento 

```{r}
  # install.packages('servr') 
  library(stringr)
  library(text2vec)
 
  frameworks2$rwn_id <- as.integer(rownames(frameworks2))

  prep_fun <- tolower
  tok_fun <- word_tokenizer

  it <- itoken(
      frameworks2$definition, 
      ids = frameworks2$rwn_id, 
      preprocessor = prep_fun, 
      tokenizer = tok_fun, 
      progressbar = TRUE
      )
  
  class(it)
  glimpse(it)
  
  vocab <- create_vocabulary(it,
      stopwords = stopwords$words) %>%
     prune_vocabulary(
      term_count_min = 1, 
      doc_proportion_max = 0.8
      )
  
  glimpse(vocab)
  
  class(vocab)
  
  vocab$term
  vocab$term_count
  vocab$doc_count %>% frq
  
  vectorizer <- vocab_vectorizer(vocab)
  
  dtm = create_dtm(it, vectorizer)
  
  dim(dtm)

  # define tfidf model
  tfidf = TfIdf$new()
  
  # fit model to train data and transform train data with fitted model
  dtm_tfidf = fit_transform(dtm, tfidf)

 lda_model = LDA$new(
   n_topics = 12, 
   doc_topic_prior = 0.1,
   topic_word_prior = 0.01
   )

  doc_topic_distr <-  
    lda_model$fit_transform(
    x = dtm_tfidf, 
    n_iter = 1000, 
    convergence_tol = 0.001, 
    n_check_convergence = 25, 
    progressbar = FALSE
    )
 
  lda_model$get_top_words(n = 12, 
    topic_number = c(1:12), lambda = 0.2)
  
    
  lda_model$plot()
    
  
```

### LDA usando bigramas 2 a 3


```{r}

  it = itoken(
      frameworks2$definition, 
      ids = frameworks2$rwn_id, 
      preprocessor = prep_fun, 
      tokenizer = tok_fun, 
      progressbar = FALSE
      )
  
  vocab = create_vocabulary(
    it, 
    stopwords = stopwords$words,
    ngram = c(3, 3)
    ) %>%
    prune_vocabulary(
      term_count_min = 1, 
      doc_proportion_max = 0.8)
  
  dim(vocab)
  
  vectorizer = vocab_vectorizer(vocab)
  dtm = create_dtm(it, vectorizer)
  
  dim(dtm)

  # define tfidf model
  tfidf = TfIdf$new()
  # fit model to train data and transform train data with fitted model
  dtm_tfidf = fit_transform(dtm, tfidf)

  dim(dtm_tfidf )

 lda_model = LDA$new(
   n_topics =8, 
   doc_topic_prior = 0.1,
   topic_word_prior = 0.01
   )

  doc_topic_distr = 
    lda_model$fit_transform(
    x = dtm_tfidf, n_iter = 1000, 
    convergence_tol = 0.001, 
    n_check_convergence = 25, 
    progressbar = FALSE
    )
   
  barplot(
    doc_topic_distr[2, ], xlab = "topic", 
    ylab = "proportion", ylim = c(0, 1), 
    names.arg = 1:ncol(doc_topic_distr)
    )
    
  lda_model$get_top_words(n = 12, 
    topic_number = c(1:4), lambda = 0.2)
  
    
  lda_model$plot()
 
```


### Classificando palavras muito parecidas 

```{r}


 load("~/Dropbox (Personal)/Ciencia de dados/ds_psicom/aulas/frameworks4.RData")



   frameworks4 <- frameworks4 %>% 
    filter(!duplicated(frameworks4[ , c(1, 2, 4)]) ) %>%
    anti_join(stopwords)
  
  names(frameworks4)
  # Análise de 100 clusters
  d   <- dist(frameworks4[ , 5:304], method="euclidean") 
  2951 / 5
  cluster  <- hclust(d, method="ward.D2")
 
  grp590 <- cutree(cluster, k = 590)
  
  
   frameworks4 <- frameworks4 %>%
     mutate(grp590 =  grp590)
   
    frameworks4 %>% 
      arrange(c) %>% 
      select(c(305, 4, 1:3)) %>% 
      view
    
    library(sjmisc)
   
    frameworks4 %>% select(grp590) %>% frq
 

```

#### Term’s inverse document frequency (idf)

_Term’s inverse document frequency (idf), which decreases the weight for commonly used words and increases the weight for words that are not used very much in a collection of documents. This can be combined with term frequency to calculate a term’s tf-idf (the two quantities multiplied together), the frequency of a term adjusted for how rarely it is used_ (p. 31)

_The idea of tf-idf is to find the important words for the content of each document by decreasing the weight for commonly used words and increasing the weight for words that are not used very much in a collection or corpus of documents_ (p. 37)

$$idf_{termo} = \mbox{ln} \frac{n_{documentos}}{n_{documentos.contendo.termo}}$$

```{r}



  frameworks4 <-  frameworks4 %>% 
   mutate(um = 1) %>%
   group_by(system, facet, words) %>%
   mutate(freq = sum(um)) %>%
   ungroup() %>%
    mutate(
     documento = str_wrap(paste(system, facet, sep = "_"), width = 23 ),
     system = factor(system)
    )
  

 
  frameworks4 <- frameworks4 %>% 
  bind_tf_idf(term = words, document = documento, n = freq)
    
  names(frameworks4)
  
  frameworks4 %>% select(-c(5:304)) %>% arrange(tf_idf) %>% view
  
  hist( frameworks4$tf_idf)
  
   frameworks4  %>% 
     ggplot(aes(x=tf_idf)) + 
     geom_histogram(color = "white", binwidth = .02) +
     scale_x_continuous(limits = c(0, 3))
   
   
```


```{r}




  frameworks5 <-  frameworks4 %>%
    filter(tf_idf > 0.08) %>%
    group_by(system, facet, documento) %>%
    summarise(
      across(d1:d300, mean, na.rm=TRUE)
     ) 
  

names(frameworks5)

  tsne <- frameworks5 %>% 
  select(3:302) %>%
  Rtsne( perplexity = 8) 
  
  frameworks5 <- frameworks5 %>% bind_cols(as.data.frame(tsne$Y))
  
  names(frameworks5)
  
  library(ggrepel)
  library(ggthemes)
  library(RColorBrewer)
  library(scales)
  
 # install.packages('unikn')  
  library(unikn)        
  

 
  cores18 <-  c(
    RColorBrewer::brewer.pal(9, "Spectral"), 
    RColorBrewer::brewer.pal(9, "Paired")
    ) 
  
  
  f3 <- ggplot(data = frameworks5,  
    mapping = aes(
      y = V1,
      x = V2,
      color = system) 
      ) +
    geom_point()  +
    geom_text_repel(
      aes(label=documento), 
        size=2, vjust=-1.2
        ) +
    theme_minimal() +
    scale_color_manual(values = cores18) +
    theme(legend.position = "none") 
  
  ggsave(filename = "f3.jpeg", device = "jpeg",
         width = 29, height = 21, units = "cm" )

```

### Exerccício

* Escolha uma base de dados com textos e faça o processo de análise descrito acima até a parte do wordcloud



